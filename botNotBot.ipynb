{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['matrixTweetsEmb_4177_100_50_300.dump',\n",
       " '.DS_Store',\n",
       " 'botNotBot.ipynb',\n",
       " '02.CNN_100x50x300D_google_0.9693.h5',\n",
       " 'botNotBot.py',\n",
       " 'Dataset-README',\n",
       " '.ipynb_checkpoints',\n",
       " '01.CNN_100x50x300D_google_0.964.h5',\n",
       " 'en',\n",
       " '.git',\n",
       " 'matrixTweetsEmb_Compressed.dump']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathEn = \"/Users/kram/Downloads/botOrNot-en_es/en\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we will create a procedure to be tested on a single file.\n",
    "\n",
    "After this first step will be completed, we will extend this procedure to create a complete dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = \"/Users/kram/Downloads/botOrNot-en_es/en/1a5b808546838869bc39cebdbad951e3.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import io\n",
    "\n",
    "def iter_docs(author):\n",
    "    '''This function extracts the text and the language from the XML'''\n",
    "    author_attr = author.attrib\n",
    "    for doc in author.iter('document'):\n",
    "        doc_dict = author_attr.copy()\n",
    "        doc_dict.update(doc.attrib)\n",
    "        doc_dict['data'] = doc.text\n",
    "        yield doc_dict\n",
    "\n",
    "xml_data = open(testfile, \"r\") # Opening the text file\n",
    "etree = ET.parse(xml_data) # Create an ElementTree object \n",
    "df = pd.DataFrame(list(iter_docs(etree.getroot()))) #Append the info to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1:7 Wherefore she went after their families: o...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>And he put his hand over the host: and they ga...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65:18 But be ye far from the Philistines.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4:29 And rose up, and went out.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24:13 My son, keep my mouth hath spoken, sayin...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30:21 And afterwards she bare unto him.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>And on the ground, and took Rebekah, and said ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13:10 And I the LORD came unto Elim: and in th...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>And his servants for his issue.</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20:3 And cast him down: deliver my people shal...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data lang\n",
       "0  1:7 Wherefore she went after their families: o...   en\n",
       "1  And he put his hand over the host: and they ga...   en\n",
       "2          65:18 But be ye far from the Philistines.   en\n",
       "3                    4:29 And rose up, and went out.   en\n",
       "4  24:13 My son, keep my mouth hath spoken, sayin...   en\n",
       "5            30:21 And afterwards she bare unto him.   en\n",
       "6  And on the ground, and took Rebekah, and said ...   en\n",
       "7  13:10 And I the LORD came unto Elim: and in th...   en\n",
       "8                    And his servants for his issue.   en\n",
       "9  20:3 And cast him down: deliver my people shal...   en"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1:7 Wherefore she went after their families: of Sered, the family of the priests, and the light shine upon thy head; for I fear the LORD.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting ID to insert in the dataframe\n",
    "\n",
    "filename = testfile.split(\"/\")[-1].split(\".\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1a5b808546838869bc39cebdbad951e3'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try to extend the procedure to the full directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time is 109.33368396759033\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Creating empty dataframe\n",
    "dataEn = pd.DataFrame()\n",
    "\n",
    "# Monitoring time to load the files\n",
    "start = time.time()\n",
    "\n",
    "for root, dirs, files in os.walk(pathEn):\n",
    "    for file in files:\n",
    "        if file == 'truth.txt':\n",
    "            continue\n",
    "        else: \n",
    "            try:\n",
    "                pathToFile = root + '/' + file # Creating path\n",
    "                # print(pathToFile) # Just for debugging\n",
    "                xml_data = open(pathToFile, \"r\", encoding=\"utf8\") # Opening the text file\n",
    "                etree = ET.parse(xml_data) # Create an ElementTree object\n",
    "                data = list(iter_docs(etree.getroot())) # Create a list of dictionaries with the data\n",
    "                filename = file.split(\".\")[0] # Get filename\n",
    "                for dictionary in data: # Loop through the dictionary\n",
    "                    dictionary['ID'] = filename # Append filename\n",
    "                dataEn = dataEn.append(data)  # Append the list of dictionary to a pandas dataframe\n",
    "                \n",
    "            # If the file is not valid, skip it\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                continue\n",
    "            \n",
    "end = time.time()\n",
    "print(\"Total running time is\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>data</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>Alex is too nice for love island :(</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>RT @STVNews: Teenager charged with rape of wom...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>@LipsTaco @jennyhastie</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>@RyanDunbar8 happy bday Ryan have the best day...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>@jennyhastie @bootywhispers I just wanna let j...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>@bootywhispers @jennyhastie what would u do ?</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>@Aidsterrr Asking for a mate :/ wee bit line o...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>Wonder what a Sunday without the fear feels like</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>RT @liamfrenchx: Brutal getn wee flashbacks a ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>Went to a&amp;amp;e last night myself after I lock...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ID  \\\n",
       "0  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "1  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "2  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "3  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "4  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "5  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "6  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "7  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "8  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "9  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "\n",
       "                                                data lang  \n",
       "0                Alex is too nice for love island :(   en  \n",
       "1  RT @STVNews: Teenager charged with rape of wom...   en  \n",
       "2                             @LipsTaco @jennyhastie   en  \n",
       "3  @RyanDunbar8 happy bday Ryan have the best day...   en  \n",
       "4  @jennyhastie @bootywhispers I just wanna let j...   en  \n",
       "5      @bootywhispers @jennyhastie what would u do ?   en  \n",
       "6  @Aidsterrr Asking for a mate :/ wee bit line o...   en  \n",
       "7   Wonder what a Sunday without the fear feels like   en  \n",
       "8  RT @liamfrenchx: Brutal getn wee flashbacks a ...   en  \n",
       "9  Went to a&amp;e last night myself after I lock...   en  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataEn.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  Alex is too nice for love island :(\n",
       "0    RT @AlexanderRugaev: The Crypto Finance Ecosys...\n",
       "0    Check out these awesome cooking t-shirts &amp;...\n",
       "0    YEWWinfo Tiny Nanoparticles to Treat a Huge Pr...\n",
       "0    Sr. Project Manager Water / Wastewater Enginee...\n",
       "0                      @lolzdonz @beckyfrancesxo !!!!!\n",
       "0    @jennycastle96 Ahaha last time acting reckless 😂😂\n",
       "0    Apr. 20/2002 - The brodway show Jesus Christ S...\n",
       "0    I'm so excited for the boojum I'm about to get...\n",
       "0    Killer bonus here 70+ page free download to he...\n",
       "0                        boooo https://t.co/Q32Ttd9FIH\n",
       "0    Create a life you love.\\n\\n#quote #life https:...\n",
       "0    @puretemerity The archaeology on Arran is amaz...\n",
       "0    Why Silicon Valley can’t fix itself\\n\\nhttps:/...\n",
       "0    @PayChen @designtaxi So you have place to put ...\n",
       "0    #GameofThrones #ThorosofMyr: There's no story....\n",
       "0    the amount of pride flags he has around him, a...\n",
       "0    @mike_mcgrail Pretty sure I seen your doppelga...\n",
       "0        Most games are lost, not won! - Casey Stengel\n",
       "0       a person wearing a hat https://t.co/UqHgJy7ji7\n",
       "0    Why the ‘Manhattan Madam’ Is Ensnared in the M...\n",
       "0    Probably going to visit the folks in T.O. in t...\n",
       "0    I painted \"Easy-going Alexander Fleming\" after...\n",
       "0    Check out this Big-Dog Pet Fountain - Keeps Fr...\n",
       "0    Just commented on @thejournal_ie: Poll: Did yo...\n",
       "0    RT @jonathansteel: Speaker from @SatAppsCatapu...\n",
       "0    Then I ran out of our database, so we rooted y...\n",
       "0    The cause of spiritual life for the spirituall...\n",
       "0    There's A New \"Deadpool 2\" Trailer And Holy He...\n",
       "0             © @5sosupdateww ] https//tco/1ukhqdtduy.\n",
       "                           ...                        \n",
       "0    RT @IainGAnderson: Glad you were able to visit...\n",
       "0    Mad how birds throw on a pair of them fake gla...\n",
       "0    Why isn't @Target's #grocery offering helping ...\n",
       "0    All arguments have two sides, but some have no...\n",
       "0    A game where you must clear out a strange test...\n",
       "0    RT @JoParkerBear: I love that rightwingers say...\n",
       "0    #goodluc….. actor... an actor. rt @xxkissmeyou...\n",
       "0    Jeffrey Tambor Is Not Returning To \"Transparen...\n",
       "0    @BerthanPete @IsThisAB0t Can't see this being ...\n",
       "0    @JohnRMoffitt I thought the trump base didn't ...\n",
       "0    In fact I would go as far to say that it’s act...\n",
       "0    Warmer Arctic temperatures are the 'new normal...\n",
       "0    On her first day back from her honeymoon, @Ste...\n",
       "0    RT @TabithaKhaye: What defines us is how well ...\n",
       "0    Excited to be able to start another season. I ...\n",
       "0    RT @MeredithFrost: Best thing you’ll see today...\n",
       "0    RT por MAS SEGUIDORES: Da RT RT SÍGUEME\\ny SIG...\n",
       "0    Senior Software Engineer Frontend – Loan Servi...\n",
       "0    I change my mind...puff-puff-give. Last candid...\n",
       "0    RT @Internet_SF: \"Member States have failed to...\n",
       "0    @Kezzang69 I can hear the Black Bull jukey cal...\n",
       "0    Here's a look at the top 2018 strategic IT bud...\n",
       "0    What you don't know won't hurt you but it prov...\n",
       "0        @CostaCoffee @moto Not been picked up yet! :(\n",
       "0    New at Affiliate Marketing?  How to Start Off ...\n",
       "0    You should check this site out if you want 117...\n",
       "0    This tweet was sent via Twitter for iPhone. I ...\n",
       "0    RT @1dish4theroad: The history of #London is r...\n",
       "0    12 Great Motivational Quotes here: http://t.co...\n",
       "0                        I think i'm going to YOLO it.\n",
       "Name: data, Length: 4120, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataEn['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>data</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>412000</td>\n",
       "      <td>412000</td>\n",
       "      <td>412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4120</td>\n",
       "      <td>373908</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>db91aa8f939b1c18a38510f589e7231d</td>\n",
       "      <td>wow, it's so sad about MJ :(</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>100</td>\n",
       "      <td>136</td>\n",
       "      <td>412000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ID                          data    lang\n",
       "count                             412000                        412000  412000\n",
       "unique                              4120                        373908       1\n",
       "top     db91aa8f939b1c18a38510f589e7231d  wow, it's so sad about MJ :(      en\n",
       "freq                                 100                           136  412000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataEn.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have merged the IDs with the data, we can create another dataframe with the labels and then merge them using the ID as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToLabels = \"/Users/kram/Downloads/botOrNot-en_es/en/truth.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kram/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "target = pd.read_csv(pathToLabels, sep=\":::\")\n",
    "target.columns=['ID', 'botOrHuman', 'sex'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>botOrHuman</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2bed15d46872169dc7deaf8d2b43a56</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25395a7dbd2caa3d828bb3dbd57d8857</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1c9f161414334b286c4dc70163744390</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1da1f87b3dc778f28268eec70ce94f19</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bd4286bfafb8a35b8e132a396b884e07</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>af61c4c017f246da69285497baf3dc0b</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7a36090b20e7bddbe55561c52f959041</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5183ff5bedcab9a6a301ff04e27166cd</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6ac10734f35a773e9f2209f8668fffdf</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b5b06752b42d3c354cc8e126f030a864</td>\n",
       "      <td>bot</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ID botOrHuman  sex\n",
       "0   2bed15d46872169dc7deaf8d2b43a56        bot  bot\n",
       "1  25395a7dbd2caa3d828bb3dbd57d8857        bot  bot\n",
       "2  1c9f161414334b286c4dc70163744390        bot  bot\n",
       "3  1da1f87b3dc778f28268eec70ce94f19        bot  bot\n",
       "4  bd4286bfafb8a35b8e132a396b884e07        bot  bot\n",
       "5  af61c4c017f246da69285497baf3dc0b        bot  bot\n",
       "6  7a36090b20e7bddbe55561c52f959041        bot  bot\n",
       "7  5183ff5bedcab9a6a301ff04e27166cd        bot  bot\n",
       "8  6ac10734f35a773e9f2209f8668fffdf        bot  bot\n",
       "9  b5b06752b42d3c354cc8e126f030a864        bot  bot"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>botOrHuman</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4119</td>\n",
       "      <td>4119</td>\n",
       "      <td>4119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4119</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>cf387a26c11e05174cb50fbea00223b0</td>\n",
       "      <td>human</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>2060</td>\n",
       "      <td>2059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ID botOrHuman   sex\n",
       "count                               4119       4119  4119\n",
       "unique                              4119          2     3\n",
       "top     cf387a26c11e05174cb50fbea00223b0      human   bot\n",
       "freq                                   1       2060  2059"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed with the concatenation of the dataframes for the English language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergedEnData = pd.merge(dataEn, target, on='ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>data</th>\n",
       "      <th>lang</th>\n",
       "      <th>botOrHuman</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>Alex is too nice for love island :(</td>\n",
       "      <td>en</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>RT @STVNews: Teenager charged with rape of wom...</td>\n",
       "      <td>en</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>@LipsTaco @jennyhastie</td>\n",
       "      <td>en</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>@RyanDunbar8 happy bday Ryan have the best day...</td>\n",
       "      <td>en</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>@jennyhastie @bootywhispers I just wanna let j...</td>\n",
       "      <td>en</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>@bootywhispers @jennyhastie what would u do ?</td>\n",
       "      <td>en</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>@Aidsterrr Asking for a mate :/ wee bit line o...</td>\n",
       "      <td>en</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>Wonder what a Sunday without the fear feels like</td>\n",
       "      <td>en</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>RT @liamfrenchx: Brutal getn wee flashbacks a ...</td>\n",
       "      <td>en</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>867be96f95dfc4e24541d19c6a5ab8bf</td>\n",
       "      <td>Went to a&amp;amp;e last night myself after I lock...</td>\n",
       "      <td>en</td>\n",
       "      <td>human</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 ID  \\\n",
       "0  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "1  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "2  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "3  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "4  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "5  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "6  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "7  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "8  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "9  867be96f95dfc4e24541d19c6a5ab8bf   \n",
       "\n",
       "                                                data lang botOrHuman     sex  \n",
       "0                Alex is too nice for love island :(   en      human  female  \n",
       "1  RT @STVNews: Teenager charged with rape of wom...   en      human  female  \n",
       "2                             @LipsTaco @jennyhastie   en      human  female  \n",
       "3  @RyanDunbar8 happy bday Ryan have the best day...   en      human  female  \n",
       "4  @jennyhastie @bootywhispers I just wanna let j...   en      human  female  \n",
       "5      @bootywhispers @jennyhastie what would u do ?   en      human  female  \n",
       "6  @Aidsterrr Asking for a mate :/ wee bit line o...   en      human  female  \n",
       "7   Wonder what a Sunday without the fear feels like   en      human  female  \n",
       "8  RT @liamfrenchx: Brutal getn wee flashbacks a ...   en      human  female  \n",
       "9  Went to a&amp;e last night myself after I lock...   en      human  female  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergedEnData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>data</th>\n",
       "      <th>lang</th>\n",
       "      <th>botOrHuman</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>411900</td>\n",
       "      <td>411900</td>\n",
       "      <td>411900</td>\n",
       "      <td>411900</td>\n",
       "      <td>411900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>4119</td>\n",
       "      <td>373808</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>db91aa8f939b1c18a38510f589e7231d</td>\n",
       "      <td>wow, it's so sad about MJ :(</td>\n",
       "      <td>en</td>\n",
       "      <td>human</td>\n",
       "      <td>bot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>100</td>\n",
       "      <td>136</td>\n",
       "      <td>411900</td>\n",
       "      <td>206000</td>\n",
       "      <td>205900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      ID                          data  \\\n",
       "count                             411900                        411900   \n",
       "unique                              4119                        373808   \n",
       "top     db91aa8f939b1c18a38510f589e7231d  wow, it's so sad about MJ :(   \n",
       "freq                                 100                           136   \n",
       "\n",
       "          lang botOrHuman     sex  \n",
       "count   411900     411900  411900  \n",
       "unique       1          2       3  \n",
       "top         en      human     bot  \n",
       "freq    411900     206000  205900  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mergedEnData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - CNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4119\n"
     ]
    }
   ],
   "source": [
    "'''Creo la litsa degli ID, delle classi e dei tweets pr ogni ID'''\n",
    "\n",
    "listaIds =[]\n",
    "listaClasses = []\n",
    "matrixTweets = []\n",
    "\n",
    "for index, x in mergedEnData.iterrows():\n",
    "    id = x['ID']\n",
    "    if id not in listaIds:\n",
    "        newList = list()\n",
    "        newList.append(x[1])\n",
    "        matrixTweets.append(newList)\n",
    "        listaIds.append(id)\n",
    "        listaClasses.append(x[3])\n",
    "    else:\n",
    "        ls = matrixTweets[listaIds.index(id)]\n",
    "        ls.append(x[1])\n",
    "        matrixTweets[listaIds.index(id)] = ls\n",
    "        \n",
    "print(len(listaIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trasformo le entità, lascio le faccine, levo le stopword e se serve agli embeddings lemmatizzo'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Trasformo le entità, lascio le faccine, levo le stopword e se serve agli embeddings lemmatizzo'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ekphrasis in /Users/kram/anaconda3/lib/python3.6/site-packages (0.5.1)\n",
      "Requirement already satisfied: termcolor in /Users/kram/anaconda3/lib/python3.6/site-packages (from ekphrasis) (1.1.0)\n",
      "Requirement already satisfied: ujson in /Users/kram/anaconda3/lib/python3.6/site-packages (from ekphrasis) (1.35)\n",
      "Requirement already satisfied: nltk in /Users/kram/anaconda3/lib/python3.6/site-packages (from ekphrasis) (3.3)\n",
      "Requirement already satisfied: ftfy in /Users/kram/anaconda3/lib/python3.6/site-packages (from ekphrasis) (5.5.1)\n",
      "Requirement already satisfied: numpy in /Users/kram/anaconda3/lib/python3.6/site-packages (from ekphrasis) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /Users/kram/anaconda3/lib/python3.6/site-packages (from ekphrasis) (4.29.1)\n",
      "Requirement already satisfied: matplotlib in /Users/kram/anaconda3/lib/python3.6/site-packages (from ekphrasis) (3.0.2)\n",
      "Requirement already satisfied: colorama in /Users/kram/anaconda3/lib/python3.6/site-packages (from ekphrasis) (0.3.9)\n",
      "Requirement already satisfied: six in /Users/kram/anaconda3/lib/python3.6/site-packages (from nltk->ekphrasis) (1.11.0)\n",
      "Requirement already satisfied: wcwidth in /Users/kram/anaconda3/lib/python3.6/site-packages (from ftfy->ekphrasis) (0.1.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/kram/anaconda3/lib/python3.6/site-packages (from matplotlib->ekphrasis) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/kram/anaconda3/lib/python3.6/site-packages (from matplotlib->ekphrasis) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/kram/anaconda3/lib/python3.6/site-packages (from matplotlib->ekphrasis) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/kram/anaconda3/lib/python3.6/site-packages (from matplotlib->ekphrasis) (2.7.3)\n",
      "Requirement already satisfied: setuptools in /Users/kram/anaconda3/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis) (39.1.0)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install ekphrasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n",
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "\n",
    "text_processor = TextPreProcessor (\n",
    "    # terms that will be normalized\n",
    "    normalize=[ 'email' , 'percent' , 'money' , 'phone' ,\n",
    "                'time' , 'url' , 'date' , 'number' ] ,\n",
    "    fix_html=True ,  # fix HTML tokens\n",
    "    segmenter=\"twitter\" ,\n",
    "    corrector=\"twitter\" ,\n",
    "    unpack_hashtags=True ,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True ,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=True ,  # spell correction for elongated words\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    dicts=[ emoticons ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transform sentences to word embeddings'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Transform sentences to word embeddings'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_300 = gensim.models.KeyedVectors.load_word2vec_format( \"/Users/kram/Downloads/botOrNot-en_es/google_w2v_300.bin\" , binary=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trasformo le frasi'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Trasformo le frasi'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer as TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import random as rn\n",
    "stop_words = set(stopwords.words('english'))\n",
    "i = 0\n",
    "matrixTweetsEmb = []\n",
    "for tweetsUser in matrixTweets:\n",
    "    embTweetsUser = []\n",
    "    if(i % 100) == 0:\n",
    "         print(i)\n",
    "    for tweet in tweetsUser:\n",
    "        embTweetUser = np.zeros([50,300], dtype=np.float16)\n",
    "        #Preprocesso\n",
    "        tokList = text_processor.pre_process_doc(tweet)\n",
    "        #Rimuovo le stopwords\n",
    "        tokList = [w for w in tokList if not w in stop_words]\n",
    "        #trovo l'embedding\n",
    "        numTok = 0;\n",
    "        for token in tokList[0:50]:\n",
    "            g_vec =[]\n",
    "            is_in_model = False\n",
    "            if token in google_300.vocab.keys ( ):\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec(token)\n",
    "            elif token == \"<number>\":\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec( \"number\")\n",
    "            elif token == \"<percent>\":\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec(\"percent\")\n",
    "            elif token == \"<money>\":\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec(\"money\")\n",
    "            elif token == \"<email>\":\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec(\"email\")\n",
    "            elif token == \"<phone>\":\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec(\"phone\")\n",
    "            elif token == \"<time>\":\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec(\"time\")\n",
    "            elif token == \"<date>\":\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec(\"date\")\n",
    "            elif token == \"<url>\":\n",
    "                is_in_model = True\n",
    "                g_vec = google_300.word_vec(\"url\")\n",
    "            elif not is_in_model:\n",
    "                max = len ( google_300.vocab.keys ( ) ) - 1\n",
    "                index = rn.randint ( 0 , max )\n",
    "                word = google_300.index2word[ index ]\n",
    "                g_vec = google_300.word_vec( word )\n",
    "\n",
    "            embTweetUser[numTok] = np.array(g_vec, dtype=np.float16)\n",
    "            numTok += 1\n",
    "        embTweetsUser.append(np.array(embTweetUser, dtype=np.float16))\n",
    "    i +=1\n",
    "    matrixTweetsEmb.append(np.array(embTweetsUser, dtype=np.float16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4119, 100, 50, 300)\n"
     ]
    }
   ],
   "source": [
    "'''Num Utenti x Num Tweets x Num MaxTokens x Dim Embedding'''\n",
    "import numpy as np \n",
    "matrixTweetsEmb = np.array(matrixTweetsEmb, dtype=np.float16)\n",
    "print(matrixTweetsEmb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['listaClasses.dump']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!{sys.executable} -m pip install joblib\n",
    "import joblib\n",
    "joblib.dump(matrixTweetsEmb,'matrixTweetsEmb_ALT.dump')\n",
    "#matrixTweetsEmb = joblib.load('matrixTweetsEmb_4177_100_50_300.dump')\n",
    "joblib.dump(listaClasses,'listaClasses.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kram/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 96, 46, 200)       1500200   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 48, 23, 200)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 44, 20, 100)       400100    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 22, 10, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 20, 8, 20)         18020     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 10, 4, 20)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 400)               320400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 2,339,222\n",
      "Trainable params: 2,339,222\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Conv2D(200,(5,5), activation ='relu', input_shape=(100,50,300)))\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Conv2D(100,(5,4), activation ='relu'))\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Conv2D(20,(3,3), activation ='relu'))\n",
    "model.add(MaxPool2D(2,2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(400, activation=\"tanh\"))\n",
    "model.add(Dense(200, activation=\"tanh\"))\n",
    "model.add(Dense(100, activation=\"tanh\"))\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'col': 0, 'mapping': [('human', 1), ('bot', 2)]}]\n"
     ]
    }
   ],
   "source": [
    "#!{sys.executable} -m pip install category_encoders\n",
    "import category_encoders as ce\n",
    "le =  ce.OneHotEncoder(return_df=False, impute_missing=False, handle_unknown=\"ignore\")\n",
    "training_classes = le.fit_transform(listaClasses)\n",
    "print(le.category_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3501 samples, validate on 618 samples\n",
      "Epoch 1/15\n",
      "3501/3501 [==============================] - 1768s 505ms/step - loss: 0.4883 - acc: 0.7721 - val_loss: 0.2417 - val_acc: 0.9061\n",
      "Epoch 2/15\n",
      "3501/3501 [==============================] - 1666s 476ms/step - loss: 0.2040 - acc: 0.9292 - val_loss: 0.2187 - val_acc: 0.9369\n",
      "Epoch 3/15\n",
      "3501/3501 [==============================] - 1653s 472ms/step - loss: 0.1142 - acc: 0.9614 - val_loss: 0.1788 - val_acc: 0.9369\n",
      "Epoch 4/15\n",
      "3501/3501 [==============================] - 1658s 474ms/step - loss: 0.0870 - acc: 0.9694 - val_loss: 0.1753 - val_acc: 0.9417\n",
      "Epoch 5/15\n",
      "3501/3501 [==============================] - 1659s 474ms/step - loss: 0.0437 - acc: 0.9843 - val_loss: 0.1882 - val_acc: 0.9531\n",
      "Epoch 6/15\n",
      "3501/3501 [==============================] - 1657s 473ms/step - loss: 0.0201 - acc: 0.9937 - val_loss: 0.1714 - val_acc: 0.9547\n",
      "Epoch 7/15\n",
      "3501/3501 [==============================] - 1657s 473ms/step - loss: 0.0209 - acc: 0.9934 - val_loss: 0.1370 - val_acc: 0.9676\n",
      "Epoch 8/15\n",
      "3501/3501 [==============================] - 1643s 469ms/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.1752 - val_acc: 0.9644\n",
      "Epoch 9/15\n",
      "3501/3501 [==============================] - 1649s 471ms/step - loss: 0.1545 - acc: 0.9580 - val_loss: 0.2036 - val_acc: 0.9353\n",
      "Epoch 10/15\n",
      "3501/3501 [==============================] - 1640s 469ms/step - loss: 0.0312 - acc: 0.9903 - val_loss: 0.1792 - val_acc: 0.9531\n",
      "Epoch 11/15\n",
      "3501/3501 [==============================] - 1648s 471ms/step - loss: 0.0163 - acc: 0.9946 - val_loss: 0.2171 - val_acc: 0.9563\n",
      "Epoch 12/15\n",
      "3501/3501 [==============================] - 1649s 471ms/step - loss: 0.0095 - acc: 0.9969 - val_loss: 0.1985 - val_acc: 0.9547\n",
      "Epoch 13/15\n",
      "3501/3501 [==============================] - 1650s 471ms/step - loss: 0.0070 - acc: 0.9977 - val_loss: 0.2302 - val_acc: 0.9547\n",
      "Epoch 14/15\n",
      "3501/3501 [==============================] - 1639s 468ms/step - loss: 0.0021 - acc: 0.9997 - val_loss: 0.2235 - val_acc: 0.9612\n",
      "Epoch 15/15\n",
      "3501/3501 [==============================] - 1640s 468ms/step - loss: 2.3948e-04 - acc: 1.0000 - val_loss: 0.1986 - val_acc: 0.9660\n"
     ]
    }
   ],
   "source": [
    "model.compile ( loss='categorical_crossentropy' , optimizer='adam' , metrics=['accuracy'] )\n",
    "\n",
    "history = model.fit(matrixTweetsEmb,training_classes,64,15,\n",
    "                      validation_split= 0.15 ,\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(history.history.keys())\n",
    "%matplotlib qt\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('02.CNN_100x50x300D_google_0.9693.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(matrixTweetsEmb,training_classes, test_size=0.15, random_state=891)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.metrics  import classification_report\n",
    "from keras.callbacks import Callback\n",
    "class MyCallBack(Callback):\n",
    "    def __init__(self,verbose=0):\n",
    "\n",
    "        super(Callback, self).__init__()\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get('val_loss')\n",
    "        # if current < 0.014:\n",
    "        #     self.model.stop_training = True\n",
    "\n",
    "        predicted = model.predict ( X_test )\n",
    "\n",
    "        test = [ '0' ] * len ( X_test )\n",
    "        i = 0\n",
    "        for cl in predicted:\n",
    "            test[ i ] = str ( np.argmax ( cl ) )\n",
    "            i += 1\n",
    "\n",
    "        test_lab = [ '0' ] * len ( X_test )\n",
    "        i = 0\n",
    "        for cl in y_test:\n",
    "            test_lab[ i ] = str ( np.argmax ( cl ) )\n",
    "            i += 1\n",
    "\n",
    "        print ( len ( X_test ) )\n",
    "        print ( classification_report ( test , test_lab ) )\n",
    "        \n",
    "        \n",
    "callbacks_list = [\n",
    "    MyCallBack(verbose=1)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3501 samples, validate on 618 samples\n",
      "Epoch 1/15\n",
      "3501/3501 [==============================] - 1826s 522ms/step - loss: 0.3299 - acc: 0.8855 - val_loss: 0.1894 - val_acc: 0.9369\n",
      "618\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.91      0.93       299\n",
      "          1       0.92      0.97      0.94       319\n",
      "\n",
      "avg / total       0.94      0.94      0.94       618\n",
      "\n",
      "Epoch 2/15\n",
      "3501/3501 [==============================] - 1657s 473ms/step - loss: 0.1466 - acc: 0.9532 - val_loss: 0.2685 - val_acc: 0.9126\n",
      "618\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.97      0.90       240\n",
      "          1       0.98      0.87      0.92       378\n",
      "\n",
      "avg / total       0.92      0.91      0.91       618\n",
      "\n",
      "Epoch 3/15\n",
      "3501/3501 [==============================] - 1643s 469ms/step - loss: 0.1137 - acc: 0.9597 - val_loss: 0.1312 - val_acc: 0.9644\n",
      "618\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.94      0.96       294\n",
      "          1       0.95      0.98      0.97       324\n",
      "\n",
      "avg / total       0.97      0.96      0.96       618\n",
      "\n",
      "Epoch 4/15\n",
      "3501/3501 [==============================] - 1624s 464ms/step - loss: 0.0491 - acc: 0.9860 - val_loss: 0.4019 - val_acc: 0.9045\n",
      "618\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.98      0.88       231\n",
      "          1       0.99      0.86      0.92       387\n",
      "\n",
      "avg / total       0.92      0.90      0.91       618\n",
      "\n",
      "Epoch 5/15\n",
      "3501/3501 [==============================] - 1613s 461ms/step - loss: 0.0390 - acc: 0.9874 - val_loss: 0.1560 - val_acc: 0.9612\n",
      "618\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.94      0.96       290\n",
      "          1       0.95      0.98      0.96       328\n",
      "\n",
      "avg / total       0.96      0.96      0.96       618\n",
      "\n",
      "Epoch 6/15\n",
      "3501/3501 [==============================] - 1610s 460ms/step - loss: 0.0097 - acc: 0.9969 - val_loss: 0.1644 - val_acc: 0.9595\n",
      "618\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.95      0.96       283\n",
      "          1       0.96      0.96      0.96       335\n",
      "\n",
      "avg / total       0.96      0.96      0.96       618\n",
      "\n",
      "Epoch 7/15\n",
      "3501/3501 [==============================] - 1673s 478ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.1970 - val_acc: 0.9595\n",
      "618\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.97      0.95       271\n",
      "          1       0.98      0.95      0.96       347\n",
      "\n",
      "avg / total       0.96      0.96      0.96       618\n",
      "\n",
      "Epoch 8/15\n",
      "3501/3501 [==============================] - 1668s 476ms/step - loss: 0.0033 - acc: 0.9991 - val_loss: 0.1829 - val_acc: 0.9693\n",
      "618\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.97      0.97       281\n",
      "          1       0.97      0.97      0.97       337\n",
      "\n",
      "avg / total       0.97      0.97      0.97       618\n",
      "\n",
      "Epoch 9/15\n",
      "1152/3501 [========>.....................] - ETA: 19:22 - loss: 0.0086 - acc: 0.9983"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7a67b9b94125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                       \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                       verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile ( loss='categorical_crossentropy' , optimizer='adam' , metrics=['accuracy'] )\n",
    "\n",
    "history = model.fit(X_train,y_train,64,15,\n",
    "                      validation_data= (X_test,y_test) ,\n",
    "                      callbacks=callbacks_list,\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "618\n",
      " Accuracy: 0.9676375404530745\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.96      0.96       284\n",
      "          1       0.97      0.97      0.97       334\n",
      "\n",
      "avg / total       0.97      0.97      0.97       618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "test = ['0']* len(X_test)\n",
    "i= 0\n",
    "for cl in predicted:\n",
    "    test[i] = str(np.argmax (cl))\n",
    "    i +=1\n",
    "\n",
    "test_lab = ['0']* len(X_test)\n",
    "i= 0\n",
    "for cl in y_test:\n",
    "    test_lab[ i ] = str(np.argmax ( cl ))\n",
    "    i += 1\n",
    "\n",
    "\n",
    "print(len(X_test))\n",
    "acc = accuracy_score(test, test_lab)\n",
    "print(\" Accuracy:\", acc)\n",
    "\n",
    "print(classification_report(test,test_lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dump() missing 1 required positional argument: 'filename'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-f43d37449c61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcompressed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrixTweetsEmb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matrixTweetsEmb_Compressed.dump'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: dump() missing 1 required positional argument: 'filename'"
     ]
    }
   ],
   "source": [
    "import zlib\n",
    "compressed = zlib.compress(matrixTweetsEmb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['matrixTweetsEmb_Compressed.dump']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(compressed,'matrixTweetsEmb_Compressed.dump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
